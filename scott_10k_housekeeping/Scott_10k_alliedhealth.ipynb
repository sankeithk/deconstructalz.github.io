{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a42d93-f0a4-4a89-83c1-27952324f5c5",
   "metadata": {},
   "source": [
    "## Scott 10K Allied health dataset\n",
    "\n",
    "### Aim: Obtain demographic, clinical and neuroimaging (T1 file paths, MWC1T1 file paths) for the scans\n",
    "\n",
    "### First order of business: create the .txt file of all ADNI T1 patients I want in my table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efc06ae-1c24-4b37-8b82-abaa4bb73096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process was interrupted.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nmydir=/rds/general/project/scott_data_adni/live/ADNI/ADNI_NIFTI/\\noutput_file=/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt\\n>\"$output_file\"\\nfind \"$mydir\" -type f -name \"*.nii.gz\" >> \"$output_file\"\\n\\nwc -l<\"${output_file}\"\\n'' died with <Signals.SIGINT: 2>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mmydir=/rds/general/project/scott_data_adni/live/ADNI/ADNI_NIFTI/\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43moutput_file=/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$output_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfind \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$mydir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m -type f -name \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.nii.gz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m >> \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$output_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mwc -l<\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$\u001b[39;49m\u001b[38;5;132;43;01m{output_file}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.13/site-packages/IPython/core/magics/script.py:159\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.13/site-packages/IPython/core/magics/script.py:327\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError while terminating subprocess (pid=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (p.pid, e))\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(p.returncode, cell) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'\\nmydir=/rds/general/project/scott_data_adni/live/ADNI/ADNI_NIFTI/\\noutput_file=/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt\\n>\"$output_file\"\\nfind \"$mydir\" -type f -name \"*.nii.gz\" >> \"$output_file\"\\n\\nwc -l<\"${output_file}\"\\n'' died with <Signals.SIGINT: 2>."
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "mydir=/rds/general/project/scott_data_adni/live/ADNI/ADNI_NIFTI/\n",
    "output_file=/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt\n",
    ">\"$output_file\"\n",
    "find \"$mydir\" -type f -name \"*.nii.gz\" >> \"$output_file\"\n",
    "\n",
    "wc -l<\"${output_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a3730-1a4c-41e6-b4f5-f4ea634fb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "t1_paths = '/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt'\n",
    "patient_id_list = []\n",
    "rid_list = []\n",
    "data_key_list = []\n",
    "t1_year_list = []\n",
    "t1_mon_list = []\n",
    "t1_day_list = []\n",
    "visdate_list = []\n",
    "\n",
    "with open(t1_paths,'r') as paths:\n",
    "    all_paths = paths.readlines()\n",
    "    for path in all_paths:\n",
    "        path_parts = path.split('/')\n",
    "        patient = path_parts[-1].strip('.nii.gz\\n')\n",
    "\n",
    "        patient_parts = patient.split('_')\n",
    "        data_key = '_'.join(patient_parts)\n",
    "        data_key_list.append(data_key)\n",
    "    \n",
    "        patient_id = '_'.join(patient_parts[:3])\n",
    "        rid = patient_id.split('_')[-1]\n",
    "        rid_list.append(rid)\n",
    "        patient_id_list.append(patient_id)\n",
    "    \n",
    "        try:\n",
    "            t1_date = patient_parts[4]\n",
    "            t1_year, t1_mon, t1_day = t1_date.split('-')\n",
    "            t1_year_list.append(t1_year)\n",
    "            t1_mon_list.append(t1_mon)\n",
    "            t1_day_list.append(t1_day)\n",
    "            visdate_list.append(f\"{t1_year}-{t1_mon}-{t1_day}\")\n",
    "        except (IndexError, ValueError):\n",
    "            t1_year_list.append(np.nan)\n",
    "            t1_mon_list.append(np.nan)\n",
    "            t1_day_list.append(np.nan)\n",
    "            visdate_list.append(np.nan)\n",
    "        \n",
    "df = {\n",
    "    \"PTID\" : patient_id_list,\n",
    "    \"RID\" : rid_list,\n",
    "    \"DATA_KEY\" : data_key_list,\n",
    "    \"T1_YEAR\" : t1_year_list,\n",
    "    \"T1_MON\" : t1_mon_list,\n",
    "    \"T1_DAY\" : t1_day_list,\n",
    "    \"VISDATE_STR\" : visdate_list\n",
    "}\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "df['T1_YEAR'] = pd.to_numeric(df['T1_YEAR'], errors='coerce')\n",
    "df['T1_MON'] = pd.to_numeric(df['T1_MON'], errors='coerce')\n",
    "df['T1_DAY'] = pd.to_numeric(df['T1_DAY'], errors='coerce')\n",
    "\n",
    "df['VISDATE'] = pd.to_datetime(df['VISDATE_STR'], errors='coerce')\n",
    "df = df.drop(columns=['VISDATE_STR'])\n",
    "\n",
    "df['EXAMDATE_4WKS_LATER'] = df['VISDATE'] + timedelta(weeks = 4)\n",
    "df['EXAMDATE_4WKS_B4'] = df['VISDATE'] - timedelta(weeks = 4)\n",
    "\n",
    "df      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d601c4-0bdb-46e3-af67-6c0064a59326",
   "metadata": {},
   "source": [
    "### Time to add T1 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0509a3-18aa-4089-b8a5-b9d6b50831a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_paths = '/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_t1_paths.txt'\n",
    "key_to_path = {}\n",
    "\n",
    "with open(t1_paths,'r') as paths:\n",
    "    all_paths = paths.readlines()\n",
    "    for path in all_paths:\n",
    "        path  = path.strip('\\n')\n",
    "        raw_key = path.split('/')[-1]\n",
    "        key = raw_key.replace('.nii.gz','')\n",
    "        key_to_path[key] = path\n",
    "\n",
    "df['T1_PATH'] = df['DATA_KEY'].map(key_to_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90055b06-7e6a-4bc8-a14c-15bc64052ef6",
   "metadata": {},
   "source": [
    "### Now for mwc1t1 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564826b-008c-450a-8297-1748693f6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mydir=/rds/general/project/c3nl_scott_students/live/data/sankeith/scott_10k_b2c/\n",
    "output_file=/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/successful_scott_10k_mwc1t1_paths.txt\n",
    "\n",
    "### collect mwc1t1 file paths##\n",
    "\n",
    ">\"$output_file\"\n",
    "find \"$mydir\" -type f -name \"mwc1t1*\" >> \"$output_file\"\n",
    "wc -l<\"${output_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e0afe-da0e-4d86-b06e-a7976f22bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwc1t1_paths = '/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/successful_scott_10k_mwc1t1_paths.txt'\n",
    "key_to_path = {}\n",
    "\n",
    "with open(mwc1t1_paths,'r') as paths:\n",
    "    all_paths = paths.readlines()\n",
    "    for path in all_paths:\n",
    "        path  = path.strip('\\n')\n",
    "        key = path.split('/')[-2]\n",
    "        key_to_path[key] = path\n",
    "\n",
    "df['MWC1T1_PATH'] = df['DATA_KEY'].map(key_to_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52203c00-4c04-4d41-96c6-7e55d25f1036",
   "metadata": {},
   "source": [
    "#### How many unique patients are there (as measured by PTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb52f2-c8ce-4f29-be92-81ae70cab9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['PTID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58f4aa-dbb7-483a-90ff-d123f813cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/interim_scott_10k_alliedhealth.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc2895-b94c-4ce1-b7b5-ebe6bce366d5",
   "metadata": {},
   "source": [
    "### Make a .m file of Greg's code, and run that. This gets us patient demographics (Gender, Date of Birth and Education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fa40c-8fa9-43b1-8688-fe5ed4248af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.m\n",
    "\n",
    "T=readtable('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/interim_scott_10k_alliedhealth.csv');\n",
    "save('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/interim_scott_10k_alliedhealth.mat');\n",
    "\n",
    "data = load('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/interim_scott_10k_alliedhealth.mat');\n",
    "head(data.T) %% shows data = struct with fields, and T = [15733 x 11]\n",
    "disp(data)\n",
    "\n",
    "%% This has most of Greg's code. I'm not going to vectorise it\n",
    "\n",
    "basepath = '/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/'\n",
    "\n",
    "vars = { ...\n",
    "    {'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/PTDEMOG_10Sep2025.csv', {'PTGENDER', 'PTDOB', 'PTEDUCAT'}, {NaN, NaT, NaN}, {[], @(x) datetime(x, 'InputFormat', 'MM/yyyy'),[] }}};\n",
    "\n",
    "for v=1:numel(vars) %%opens the relevant CSV, and makes sure that VISDATE is a column. If not, then it duplicates EXAMDATE and calls it VISDATE.\n",
    "    Talli = readtable(vars{v}{1});\n",
    "    Talli.PTID = string(Talli.PTID);\n",
    "    if(~ismember('VISDATE', Talli.Properties.VariableNames))\n",
    "        Talli.VISDATE = Talli.EXAMDATE;\n",
    "    end\n",
    "   \n",
    "    fprintf('joining file %s, rows=%d\\n', vars{v}{1}, height(Talli));\n",
    "    \n",
    "    varNames = vars{v}{2}; %%selects the appropriate columns from the relevant CSV\n",
    "    if(~isempty(vars{v}{3}))\n",
    "        fprintf('(setting types)\\n');\n",
    "        for vf=1:numel(vars{v}{3})\n",
    "             T{:,varNames{vf}} = repmat(vars{v}{3}{vf}, height(T), 1); %% padding missing values with NaNs or NaTs\n",
    "        end\n",
    "    else\n",
    "        T{:,varNames} = nan(height(T),numel(varNames)); %%also padding missing values, but this is for the case where there a column entirely contains missing values\n",
    "    end\n",
    "    \n",
    "    if(~isempty(vars{v}{4})) %%setting the appropriate date format (e.g., datetime)\n",
    "        fprintf('(converting)\\n');\n",
    "        for vf=1:numel(vars{v}{4})\n",
    "            if(~isempty(vars{v}{4}{vf}))\n",
    "                Talli.(vars{v}{2}{vf}) = vars{v}{4}{vf}(Talli.(vars{v}{2}{vf}));\n",
    "            end\n",
    "        end\n",
    "        fprintf('(done)\\n')\n",
    "    end\n",
    "\n",
    "    for s=1:height(T)\n",
    "        Tsub = Talli(Talli.PTID == T.PTID(s),:);\n",
    "        if(~isempty(Tsub))\n",
    "            Tsub.datediffs = abs(days( (Tsub.VISDATE - T.VISDATE(s))));\n",
    "            Tsub = sortrows(Tsub, 'datediffs', 'asc'); % order so that the one nearest the imaging appears 1st\n",
    "            T(s, varNames) = Tsub(1,varNames);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "fprintf('Width: %d Height: %d\\n', width(T), height(T))\n",
    "writetable(T, '/rds/general/project/c3nl_scott_students/ephemeral/sankeith/gregbased_scott10k_alliedhealth.csv')\n",
    "\n",
    "if isfile('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/gregbased_scott10k_alliedhealth.csv')\n",
    "    disp('Demographics data joined')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee299d-7ffd-4920-abba-49e59b0c5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "module load tools/prod\n",
    "module --ignore_cache load MATLAB2024/b > /dev/null 2>&1\n",
    "\n",
    "matlab -nosplash -nodesktop -r \"run('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.m'); exit\"\n",
    "\n",
    "rm /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.* \n",
    "rm -f ~/java* ~/*crash*dump*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3bcb5-a105-4a76-951b-377ca1d071ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "chmod -Rf 775 /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.*\n",
    "\n",
    "/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_add_demographics_info.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f00cc-965e-4b66-83c5-a3d03011a783",
   "metadata": {},
   "source": [
    "### Load in time-sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2471cf-c6a2-4ea9-8319-e7f2b188ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import fnmatch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4595e21c-f6a0-44e1-a2f9-b470126ddbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15733\n",
      "15\n",
      "2635\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/gregbased_scott10k_alliedhealth.csv', low_memory = False)\n",
    "df[f'VISDATE'] = pd.to_datetime(df[f'VISDATE'])\n",
    "df[f'EXAMDATE_4WKS_LATER'] = pd.to_datetime(df[f'EXAMDATE_4WKS_LATER'])\n",
    "df[f'EXAMDATE_4WKS_B4'] = pd.to_datetime(df[f'EXAMDATE_4WKS_B4'])\n",
    "df[f'PTDOB'] = pd.to_datetime(df[f'PTDOB'])\n",
    "df[f'PTAGE_YEARS'] = df['VISDATE'] - df['PTDOB']\n",
    "df[f'PTAGE'] = df[f'PTAGE_YEARS'] / timedelta(days = 365)\n",
    "df.drop(columns = 'PTAGE_YEARS', inplace = True)\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(len(df['PTID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab750bf-311e-47bb-9996-0ffe002bdb91",
   "metadata": {},
   "source": [
    "### Merge APOE Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc67bad-a51c-4eb1-ba4b-0a0096360e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            PTID GENOTYPE\n",
      "0     011_S_0002      3/3\n",
      "1     011_S_0003      3/4\n",
      "2     022_S_0004      3/3\n",
      "3     011_S_0005      3/3\n",
      "4     022_S_0007      3/4\n",
      "...          ...      ...\n",
      "2755  341_S_7018      3/3\n",
      "2756  035_S_7019      3/3\n",
      "2757  052_S_7027      4/4\n",
      "2758  941_S_7041      3/4\n",
      "2759  035_S_7049      3/3\n",
      "\n",
      "[2760 rows x 2 columns]\n",
      "15733\n"
     ]
    }
   ],
   "source": [
    "apoe_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/APOERES_28Mar2025.csv',low_memory = False)[['PTID','GENOTYPE']]\n",
    "apoe_df.drop_duplicates(subset = ['PTID','GENOTYPE'], inplace = True)\n",
    "print(apoe_df)\n",
    "df = df.merge(apoe_df, how = 'left', on = ['PTID'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999a81d-1450-400f-a1d4-cf945e596013",
   "metadata": {},
   "source": [
    "### Prepping ADAS13 DataFrame and MRI FIELD STRENGTH DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ac80fa-cbbc-4cd6-a868-cc0d6c3abcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11618\n",
      "13933\n",
      "742\n"
     ]
    }
   ],
   "source": [
    "### Prepping ADAS123 DataFrame ###\n",
    "adas_adni1_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/ADASSCORES_01Apr2025.csv', low_memory = False)[['PTID','EXAMDATE','TOTALMOD']]\n",
    "adas_adni1_df.rename(columns={'TOTALMOD': 'TOTAL13', 'EXAMDATE': 'VISDATE'}, inplace=True)\n",
    "\n",
    "adas_adnigo23_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/ADAS_ADNIGO23_31Mar2025.csv', low_memory = False)\n",
    "adas_adnigo23_df = adas_adnigo23_df[['PTID','VISDATE','TOTAL13']]\n",
    "\n",
    "adas_df = pd.concat([adas_adni1_df, adas_adnigo23_df], ignore_index=True)\n",
    "\n",
    "print(len(adas_df))\n",
    "\n",
    "adas_df.to_csv(f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/ADAS13_{datetime.datetime.now().strftime('%d%b%Y')}.csv', index = False)\n",
    "\n",
    "### Prepping MRI FIELD STRENGTH DataFrame ###\n",
    "\n",
    "mrimeta_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MRIMETA_11Sep2025.csv', low_memory = False)[['PTID','EXAMDATE','FIELD_STRENGTH']]\n",
    "mrimeta_3t_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MRI3META_11Sep2025.csv', low_memory = False)[['PTID','EXAMDATE','FIELD_STRENGTH']]\n",
    "mri_df = pd.concat([mrimeta_df, mrimeta_3t_df], ignore_index=True)\n",
    "print(len(mri_df))\n",
    "mri_df.to_csv(f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MRIFLDSTRNGTH_{datetime.datetime.now().strftime('%d%b%Y')}.csv', index = False)\n",
    "\n",
    "### Prepping Amyloid DataFrame - I only use the batemanlab info which includes EXAMDATE (the other batemanlab DataFrame only has VISCODE, so can't join with Scott 10K) ###\n",
    "\n",
    "amyloid_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/batemanlab_20221118_31Mar2025.csv', low_memory = False)[['RID' ,'EXAMDATE', 'Abeta_4240_Standardized', 'Intertcept_Standardization', 'Slope_Standardization', 'Sample_volume', 'Sample_volume_UNITS', 'Abeta_42_conc', 'Abeta_42_conc_UNITS', 'Abeta_42_N14N15', 'Abeta_42_N15_ISTD_amount', 'Abeta_42_N15_ISTD_amount_UNITS', 'Abeta_40_conc', 'Abeta_40_conc_UNITS', 'Abeta_40_N14N15', 'Abeta_40_N15_ISTD_amount', 'Abeta_40_N15_ISTD_amount_UNITS', 'Abeta_4240']]\n",
    "print(len(amyloid_df))\n",
    "amyloid_df.to_csv(f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/AMYLOID_{datetime.datetime.now().strftime('%d%b%Y')}.csv', index = False)\n",
    "\n",
    "### Prepping p217 tau DataFrame ###\n",
    "\n",
    "tau_df = pd.read_csv('/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/JANSSEN_PLASMA_P217_TAU_01Apr2025.csv', low_memory = False)[['PTID', 'EXAMDATE', 'DILUTION_CORRECTED_CONC', 'CV']]\n",
    "tau_df.rename(columns={'DILUTION_CORRECTED_CONC': 'P217_DILUTION_CORRECTED_CONC', 'CV': 'P217_CV'}, inplace=True)\n",
    "tau_df.to_csv(f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/P217_{datetime.datetime.now().strftime('%d%b%Y')}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779bc3a8-b463-4168-bd5b-27d9ae30b38b",
   "metadata": {},
   "source": [
    "### Merging time-sensitive visit information (based on Greg's code):\n",
    "1. Load an allied health file\n",
    "2. Ensure allied health file as examdate\n",
    "3. Do a right type join (assume left = og file, right = allied health file)\n",
    "4. Extract rows only with a T1 file path, so we have full rows\n",
    "5. Calculate abs(allied health data - MRI visit date) and label it like df.datediffs or some shit\n",
    "6. df.groupby with PTID, and then sort by datediffs ascend\n",
    "7. Keep the row where datadiff is less than 28 days and the smallest val. If tie, then we ball and just choose at random\n",
    "8. Conduct a final length of rows check - MUST BE 15733\n",
    "9. Rinse and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c65eb-3a0f-40db-83ea-fb0792a912c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of MRIFLDSTRNGTH: 13933\n",
      "Length of the freshly joined dataframe: 118218\n",
      "After dropping rows without T1 paths: 118218\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_MRIFLDSTRNGTH are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_MRIFLDSTRNGTH > 28 days *OR* there is no EXAMDATE_MRIFLDSTRNGTH column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 16804. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 16796.\n",
      "There are 2126 rows with identical T1 MRI file paths, from 1063 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    15640.000000\n",
      "mean         0.037148\n",
      "std          0.697452\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         28.000000\n",
      "Name: DATEDIFFS_MRIFLDSTRNGTH, dtype: float64\n",
      "\n",
      "Length of DXSUM: 15674\n",
      "Length of the freshly joined dataframe: 112212\n",
      "After dropping rows without T1 paths: 112212\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_DXSUM are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_DXSUM > 28 days *OR* there is no EXAMDATE_DXSUM column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15811. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15768.\n",
      "There are 70 rows with identical T1 MRI file paths, from 35 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    11932.000000\n",
      "mean         8.138451\n",
      "std          7.818778\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          6.000000\n",
      "75%         14.000000\n",
      "max         28.000000\n",
      "Name: DATEDIFFS_DXSUM, dtype: float64\n",
      "\n",
      "Length of MMSE: 14119\n",
      "Length of the freshly joined dataframe: 105119\n",
      "After dropping rows without T1 paths: 105119\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_MMSE are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_MMSE > 28 days *OR* there is no EXAMDATE_MMSE column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15755. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15733.\n",
      "There are 0 rows with identical T1 MRI file paths, from 0 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    13148.000000\n",
      "mean         5.750532\n",
      "std          7.505814\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%         10.000000\n",
      "max         28.000000\n",
      "Name: DATEDIFFS_MMSE, dtype: float64\n",
      "\n",
      "Length of MOCA: 8466\n",
      "Length of the freshly joined dataframe: 66487\n",
      "After dropping rows without T1 paths: 66487\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_MOCA are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_MOCA > 28 days *OR* there is no EXAMDATE_MOCA column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15745. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15733.\n",
      "There are 0 rows with identical T1 MRI file paths, from 0 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    6649.000000\n",
      "mean        5.625808\n",
      "std         7.579711\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%        10.000000\n",
      "max        28.000000\n",
      "Name: DATEDIFFS_MOCA, dtype: float64\n",
      "\n",
      "Length of NEUROBAT: 16837\n",
      "Length of the freshly joined dataframe: 120370\n",
      "After dropping rows without T1 paths: 120370\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_NEUROBAT are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_NEUROBAT > 28 days *OR* there is no EXAMDATE_NEUROBAT column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15877. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15847.\n",
      "There are 228 rows with identical T1 MRI file paths, from 114 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    13737.000000\n",
      "mean         5.438233\n",
      "std          6.934756\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          9.000000\n",
      "max         28.000000\n",
      "Name: DATEDIFFS_NEUROBAT, dtype: float64\n",
      "\n",
      "Length of CDR: 14102\n",
      "Length of the freshly joined dataframe: 107905\n",
      "After dropping rows without T1 paths: 107905\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_CDR are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_CDR > 28 days *OR* there is no EXAMDATE_CDR column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15755. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15733.\n",
      "There are 0 rows with identical T1 MRI file paths, from 0 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n",
      "Final row count: 15733\n",
      "Remaining duplicates: 0\n",
      "1    15733\n",
      "Name: count, dtype: int64\n",
      "count    13070.000000\n",
      "mean         5.910023\n",
      "std          7.561881\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%         11.000000\n",
      "max         28.000000\n",
      "Name: DATEDIFFS_CDR, dtype: float64\n",
      "\n",
      "Length of FAQ: 12734\n",
      "Length of the freshly joined dataframe: 108123\n",
      "After dropping rows without T1 paths: 108123\n",
      "Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_FAQ are at the top\n",
      "Finding rows where *EITHER* DATE_DIFFS_FAQ > 28 days *OR* there is no EXAMDATE_FAQ column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\n",
      "Length of database before dropping exactly identical rows = 15750. Tsub is of type <class 'pandas.core.frame.DataFrame'>\n",
      "Length of database after dropping rows which are exactly identical = 15734.\n",
      "There are 2 rows with identical T1 MRI file paths, from 1 different MRI scans\n",
      "Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\n"
     ]
    }
   ],
   "source": [
    "vars = [\n",
    "    [f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MRIFLDSTRNGTH_{datetime.datetime.now().strftime('%d%b%Y')}.csv',['PTID','EXAMDATE','FIELD_STRENGTH'], {'FIELD_STRENGTH': pd.NA}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/DXSUM_10Sep2025.csv',['PTID','EXAMDATE','PHASE', 'DIAGNOSIS'], {'PHASE': pd.NA,'DIAGNOSIS':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MMSE_28Mar2025.csv', ['PTID','VISDATE','MMSCORE'], {'MMSCORE':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MOCA_30Mar2025.csv', ['PTID','VISDATE', 'MOCA'], {'MOCA':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/NEUROBAT_28Mar2025.csv', ['PTID','VISDATE','LIMMTOTAL', 'CLOCKSCOR', 'LDELTOTAL', 'LDELCUE', 'ANART'], {'LIMMTOTAL':np.nan, 'CLOCKSCOR':np.nan, 'LDELTOTAL':np.nan, 'LDELCUE':np.nan, 'ANART':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/CDR_28Mar2025.csv', ['PTID','VISDATE', 'CDGLOBAL'], {'CDGLOBAL':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/FAQ_28Mar2025.csv', ['PTID','VISDATE', 'FAQTOTAL'], {'FAQTOTAL':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/NPIQ_28Mar2025.csv',['PTID', 'VISDATE','NPISCORE'], {'NPISCORE':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/GDSCALE_28Mar2025.csv',['PTID','VISDATE','GDTOTAL'], {'GDTOTAL':np.nan}],\n",
    "    ['/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/MODHACH_01Apr2025.csv',['PTID', 'VISDATE','HMSCORE'], {'HMSCORE':np.nan}],\n",
    "    [f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/ADAS13_{datetime.datetime.now().strftime('%d%b%Y')}.csv',['PTID','VISDATE','TOTAL13'], {'TOTAL13':np.nan}],\n",
    "    [f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/P217_{datetime.datetime.now().strftime('%d%b%Y')}.csv',['PTID', 'EXAMDATE', 'P217_DILUTION_CORRECTED_CONC', 'P217_CV'], {'P217_DILUTION_CORRECTED_CONC': np.nan, 'P217_CV': np.nan}]\n",
    "]\n",
    "for i in range(0,len(vars)):\n",
    "    column_suffix = vars[i][0].split('/')[-1]\n",
    "    column_suffix = column_suffix.split('_')[0]\n",
    "    varnames = vars[i][1]\n",
    "    \n",
    "    #Load an allied health file\n",
    "    Talli = pd.read_csv(vars[i][0], low_memory = False)\n",
    "    Talli = Talli[vars[i][1]]\n",
    "    \n",
    "    if 'RID' in Talli.columns:\n",
    "        Talli.drop(columns = 'RID', inplace = True)    \n",
    "    print(f\"Length of {column_suffix}: {len(Talli)}\")\n",
    "    \n",
    "    #Ensure allied health file as examdate\n",
    "    if 'VISDATE' in Talli.columns:\n",
    "        Talli.rename(columns = {'VISDATE':f\"EXAMDATE_{column_suffix}\"}, inplace = True)\n",
    "        Talli[f'EXAMDATE_{column_suffix}'] = pd.to_datetime(Talli[f'EXAMDATE_{column_suffix}'], errors = 'coerce')\n",
    "    else:\n",
    "        Talli[f'EXAMDATE'] = pd.to_datetime(Talli[f'EXAMDATE'], errors = 'coerce')\n",
    "        Talli.rename(columns = {'EXAMDATE': f'EXAMDATE_{column_suffix}'}, inplace = True)\n",
    "\n",
    "    #Do a left type join\n",
    "    Tsub = pd.merge(df,Talli, on = 'PTID', how = 'left')\n",
    "    print(f\"Length of the freshly joined dataframe: {len(Tsub)}\")\n",
    "    \n",
    "    #Extract rows only with a T1 file path, so we have full rows\n",
    "    Tsub.dropna(subset='T1_PATH',inplace=True)\n",
    "    print(f\"After dropping rows without T1 paths: {len(Tsub)}\")\n",
    "\n",
    "    #Calculate abs(allied health data - MRI visit date) and label it like df.datediffs or some shit\n",
    "    try:\n",
    "        Tsub[f'DATEDIFFS_{column_suffix}'] = abs(Tsub[f'EXAMDATE_{column_suffix}'] - Tsub['VISDATE'])\n",
    "    except KeyError:\n",
    "        Tsub[f'DATEDIFFS_{column_suffix}'] = abs(Tsub['EXAMDATE'] - Tsub['VISDATE'])\n",
    "\n",
    "    #df.groupby with PTID, and then sort by datediffs ascend. Select the/a row where DATEDIFFS is the smallest\n",
    "    Tsub = Tsub.sort_values(f'DATEDIFFS_{column_suffix}', ascending=True)\n",
    "\n",
    "    print(f\"Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_{column_suffix} are at the top\") \n",
    "    min_diffs = (\n",
    "    Tsub\n",
    "    .groupby('T1_PATH')[f'DATEDIFFS_{column_suffix}']\n",
    "    .transform('min')\n",
    "    )\n",
    "    \n",
    "    Tsub = Tsub[\n",
    "    (Tsub[f'DATEDIFFS_{column_suffix}'] == min_diffs) |\n",
    "    (min_diffs.isna() & Tsub[f'DATEDIFFS_{column_suffix}'].isna())\n",
    "    ]\n",
    "\n",
    "    Tsub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"Finding rows where *EITHER* DATE_DIFFS_{column_suffix} > 28 days *OR* there is no EXAMDATE_{column_suffix} column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\") \n",
    "    mask = (Tsub[f'DATEDIFFS_{column_suffix}'] > pd.Timedelta(28, \"D\")) | (Tsub[f'EXAMDATE_{column_suffix}'].isna() == True)\n",
    "    \n",
    "    Tsub.loc[mask, f'DATEDIFFS_{column_suffix}'] = pd.NaT\n",
    "    Tsub.loc[mask, f'EXAMDATE_{column_suffix}'] = pd.NaT\n",
    "    \n",
    "    for key in vars[i][2].keys():\n",
    "        Tsub.loc[mask, key] = vars[i][2].get(key)\n",
    "    print(f'Length of database before dropping exactly identical rows = {len(Tsub)}. Tsub is of type {type(Tsub)}')\n",
    "    Tsub.drop_duplicates(keep = 'first',inplace = True)\n",
    "    print(f'Length of database after dropping rows which are exactly identical = {len(Tsub)}.')\n",
    "    \n",
    "    dupe_mask = Tsub.duplicated(subset = 'T1_PATH',keep=False)\n",
    "    Tsub_dupes = Tsub.loc[dupe_mask].copy()\n",
    "    print(f'There are {len(Tsub_dupes)} rows with identical T1 MRI file paths, from {Tsub_dupes['T1_PATH'].nunique()} different MRI scans')\n",
    "\n",
    "    def fewest_missing_one(group):\n",
    "        missing_counts = group.isna().sum(axis=1)\n",
    "        min_missing = missing_counts.min()\n",
    "        candidates = group[missing_counts == min_missing]\n",
    "        # if more than one, pick the first (or the row with smallest DATEDIFF)\n",
    "        return candidates.iloc[[0]]  # keeps only 1 row\n",
    "\n",
    "    print(\"Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\")\n",
    "    Tsub = Tsub.groupby('T1_PATH', group_keys=False).apply(fewest_missing_one, include_groups = True)\n",
    "    Tsub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Final sanity check\n",
    "    dup_cols = [c for c in ['T1_PATH', 'MWC1T1_PATH'] if c in Tsub.columns]\n",
    "    print(f\"Final row count: {len(Tsub)}\")\n",
    "    print(f\"Remaining duplicates: {Tsub.duplicated(subset=dup_cols).sum()}\")\n",
    "    \n",
    "    # For each T1_PATH, see how many rows were kept\n",
    "    rows_per_scan = Tsub.groupby('T1_PATH').size()\n",
    "    print(rows_per_scan.value_counts())\n",
    "\n",
    "    #Make the DATEDIFFS column numeric? Just because as datetime it says 'days' at the end and I don't like that.\n",
    "    Tsub[f'DATEDIFFS_{column_suffix}'] = (Tsub[f'DATEDIFFS_{column_suffix}'].dt.days)\n",
    "    \n",
    "    print(Tsub[f'DATEDIFFS_{column_suffix}'].describe())\n",
    "    print('')\n",
    "\n",
    "    Tsub.to_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/final_gregbased_scott10k_alliedhealth.csv', index = False)\n",
    "    df = Tsub\n",
    "\n",
    "Tsub.to_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/scott10k_alliedhealth.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9743afa-687e-4ca9-be03-1d333b098d42",
   "metadata": {},
   "source": [
    "### Add Amyloid data (use RID to join, and only one of the amyloid spreadsheets since the other doesn't have EXAMDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ab453-8ad8-4a6f-a078-908ad2efbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "df = pd.read_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/scott10k_alliedhealth.csv', low_memory = True)\n",
    "df['VISDATE'] = pd.to_datetime(df['VISDATE'], errors='coerce')\n",
    "\n",
    "\n",
    "vars = [\n",
    "    [f'/rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott_10k_extra_adni_info/AMYLOID_{datetime.datetime.now().strftime('%d%b%Y')}.csv',\n",
    "     ['RID' ,'EXAMDATE', 'Abeta_4240_Standardized', 'Intertcept_Standardization', 'Slope_Standardization', 'Sample_volume', 'Sample_volume_UNITS', 'Abeta_42_conc', 'Abeta_42_conc_UNITS', 'Abeta_42_N14N15', 'Abeta_42_N15_ISTD_amount', 'Abeta_42_N15_ISTD_amount_UNITS', 'Abeta_40_conc', 'Abeta_40_conc_UNITS', 'Abeta_40_N14N15', 'Abeta_40_N15_ISTD_amount', 'Abeta_40_N15_ISTD_amount_UNITS', 'Abeta_4240'], \n",
    "     {'Abeta_4240_Standardized': np.nan, \n",
    "     'Intertcept_Standardization': np.nan,\n",
    "     'Slope_Standardization': np.nan,\n",
    "     'Sample_volume': np.nan,\n",
    "     'Sample_volume_UNITS': pd.NA,\n",
    "     'Abeta_42_conc': np.nan,\n",
    "     'Abeta_42_conc_UNITS': pd.NA,\n",
    "     'Abeta_42_N14N15': np.nan,\n",
    "     'Abeta_42_N15_ISTD_amount': np.nan,\n",
    "     'Abeta_42_N15_ISTD_amount_UNITS': pd.NA,\n",
    "     'Abeta_40_conc': np.nan,\n",
    "     'Abeta_40_conc_UNITS': pd.NA,\n",
    "     'Abeta_40_N14N15': np.nan,\n",
    "     'Abeta_40_N15_ISTD_amount': np.nan,\n",
    "     'Abeta_40_N15_ISTD_amount_UNITS': pd.NA,\n",
    "     'Abeta_4240': np.nan}]\n",
    "]\n",
    "for i in range(0,len(vars)):\n",
    "    column_suffix = vars[i][0].split('/')[-1]\n",
    "    column_suffix = column_suffix.split('_')[0]\n",
    "    varnames = vars[i][1]\n",
    "    \n",
    "    #Load an allied health file\n",
    "    Talli = pd.read_csv(vars[i][0], low_memory = False)\n",
    "    Talli = Talli[vars[i][1]]\n",
    "    \n",
    "    #Ensure allied health file as examdate\n",
    "    Talli[f'EXAMDATE'] = pd.to_datetime(Talli[f'EXAMDATE'], errors = 'coerce')\n",
    "    Talli.rename(columns = {'EXAMDATE': f'EXAMDATE_{column_suffix}'}, inplace = True)\n",
    "\n",
    "    #Do a left type join\n",
    "    Tsub = pd.merge(df, Talli, on = 'RID', how = 'left')\n",
    "    print(f\"Length of the freshly joined dataframe: {len(Tsub)}\")\n",
    "    \n",
    "    #Extract rows only with a T1 file path, so we have full rows\n",
    "    Tsub.dropna(subset='T1_PATH',inplace=True)\n",
    "    print(f\"After dropping rows without T1 paths: {len(Tsub)}\")\n",
    "\n",
    "    #Calculate abs(allied health data - MRI visit date) and label it like df.datediffs or some shit\n",
    "    Tsub[f'DATEDIFFS_{column_suffix}'] = abs(Tsub[f'EXAMDATE_{column_suffix}'] - Tsub['VISDATE'])\n",
    "\n",
    "    #df.groupby with PTID, and then sort by datediffs ascend. Select the/a row where DATEDIFFS is the smallest\n",
    "    Tsub = Tsub.sort_values(f'DATEDIFFS_{column_suffix}', ascending=True)\n",
    "\n",
    "    print(f\"Grouping rows by 'T1_PATH'. Within groups, organising rows so smallest DATE_DIFFS_{column_suffix} are at the top\") \n",
    "    min_diffs = (\n",
    "    Tsub\n",
    "    .groupby('T1_PATH')[f'DATEDIFFS_{column_suffix}']\n",
    "    .transform('min')\n",
    "    )\n",
    "    \n",
    "    Tsub = Tsub[\n",
    "    (Tsub[f'DATEDIFFS_{column_suffix}'] == min_diffs) |\n",
    "    (min_diffs.isna() & Tsub[f'DATEDIFFS_{column_suffix}'].isna())\n",
    "    ]\n",
    "\n",
    "    Tsub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"Finding rows where *EITHER* DATE_DIFFS_{column_suffix} > 28 days *OR* there is no EXAMDATE_{column_suffix} column. If that's the case, replace the relevant values with NaNs, NaTs and pd.NAs\") \n",
    "    mask = (Tsub[f'DATEDIFFS_{column_suffix}'] > pd.Timedelta(28, \"D\")) | (Tsub[f'EXAMDATE_{column_suffix}'].isna() == True)\n",
    "    \n",
    "    Tsub.loc[mask, f'DATEDIFFS_{column_suffix}'] = pd.NaT\n",
    "    Tsub.loc[mask, f'EXAMDATE_{column_suffix}'] = pd.NaT\n",
    "    \n",
    "    for key in vars[i][2].keys():\n",
    "        Tsub.loc[mask, key] = vars[i][2].get(key)\n",
    "\n",
    "    print(f'Length of database before dropping exactly identical rows = {len(Tsub)}. Tsub is of type {type(Tsub)}')\n",
    "    Tsub.drop_duplicates(keep = 'first',inplace = True)\n",
    "    print(f'Length of database after dropping rows which are exactly identical = {len(Tsub)}.')\n",
    "    \n",
    "    dupe_mask = Tsub.duplicated(subset = 'T1_PATH',keep=False)\n",
    "    Tsub_dupes = Tsub.loc[dupe_mask].copy()\n",
    "    print(f'There are {len(Tsub_dupes)} rows with identical T1 MRI file paths, from {Tsub_dupes['T1_PATH'].nunique()} different MRI scans')\n",
    "\n",
    "    def fewest_missing_one(group):\n",
    "        missing_counts = group.isna().sum(axis=1)\n",
    "        min_missing = missing_counts.min()\n",
    "        candidates = group[missing_counts == min_missing]\n",
    "        # if more than one, pick the first (or the row with smallest DATEDIFF)\n",
    "        return candidates.iloc[[0]]  # keeps only 1 row\n",
    "\n",
    "    print(\"Grouping rows by T1_PATH, finding the rows with the fewest missing values. If it's a tie between two rows, the first one for each T1_PATH group is kept\")\n",
    "    Tsub = Tsub.groupby('T1_PATH', group_keys=False).apply(fewest_missing_one, include_groups = True)\n",
    "    Tsub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Final sanity check\n",
    "    dup_cols = [c for c in ['T1_PATH', 'MWC1T1_PATH'] if c in Tsub.columns]\n",
    "    print(f\"Final row count: {len(Tsub)}\")\n",
    "    print(f\"Remaining duplicates: {Tsub.duplicated(subset=dup_cols).sum()}\")\n",
    "    \n",
    "    # For each T1_PATH, see how many rows were kept\n",
    "    rows_per_scan = Tsub.groupby('T1_PATH').size()\n",
    "    print(rows_per_scan.value_counts())\n",
    "\n",
    "    #Make the DATEDIFFS column numeric? Just because as datetime it says 'days' at the end and I don't like that.\n",
    "    Tsub[f'DATEDIFFS_{column_suffix}'] = (Tsub[f'DATEDIFFS_{column_suffix}'].dt.days)\n",
    "    \n",
    "    print(Tsub[f'DATEDIFFS_{column_suffix}'].describe())\n",
    "    print('')\n",
    "\n",
    "    Tsub.to_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/withamyloid_gregbased_scott10k_alliedhealth.csv', index = False)\n",
    "    df = Tsub\n",
    "\n",
    "Tsub.to_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/debug_scott10k_alliedhealth.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca27a36-ac49-4f2c-9a1a-ea74a0395623",
   "metadata": {},
   "source": [
    "### Checking diagnoses of scans in Scott 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a315e-a6bc-4f8f-b19a-9e20621a2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/rds/general/project/c3nl_scott_students/ephemeral/sankeith/debug_scott10k_alliedhealth.csv', low_memory = False)\n",
    "\n",
    "print(df['DIAGNOSIS'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8865e36-8018-42e2-8768-5162ee0a6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rsync -av /rds/general/project/c3nl_scott_students/ephemeral/sankeith/debug_scott10k_alliedhealth.csv /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping/scott10k_alliedhealth.csv\n",
    "cd /rds/general/project/c3nl_scott_students/live/sankeith/scott_10k_housekeeping\n",
    "git add scott10k_alliedhealth.csv\n",
    "dategcp=\"`date +%d%b%Y`\"\n",
    "datetime=\"`date +%H%M%S`\"\n",
    "message=\"Remade Scott 10K allied health database: ${dategcp}, ${datetime}\"\n",
    "\n",
    "git commit -a -m \"$message\"\n",
    "git push -u origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_1)",
   "language": "python",
   "name": "venv_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
